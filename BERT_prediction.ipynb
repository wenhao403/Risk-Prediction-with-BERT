{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aHpbvYCTSnE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tfqECqQp5BBv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from random import sample, seed\n",
        "import random\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import BertConfig\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "J-PxX6lC3692"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/屏東孤老/'\n",
        "# dat = pd.read_csv(os.path.join(path, '(加風險)109-110字串化.csv'))\n",
        "dat = pd.read_csv(os.path.join(path, '(加風險)109-111字串化.csv'))\n",
        "print(dat.shape)\n",
        "# rename cols\n",
        "dat.columns = ['ID', 'text', 'label']\n",
        "# drop duplicates\n",
        "dat = dat.drop_duplicates(subset=['ID'], keep='last')\n",
        "dat = dat.reset_index(drop=True)\n",
        "print(dat.shape)\n",
        "print(Counter(dat['label']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kmaBy9a6EyO",
        "outputId": "8acf5e80-62d2-4743-d9b8-37fddbc9e090"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5540, 3)\n",
            "(4201, 3)\n",
            "Counter({'低風險': 3884, '中高風險': 317})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "death_ID2label = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/屏東孤老/death_ID2label.csv')\n",
        "# update new labels\n",
        "dat.loc[dat['ID'].isin(death_ID2label['ID']), 'label'] = '中高風險'\n",
        "print(Counter(dat['label']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVWoCiDAWWRU",
        "outputId": "1a050f88-d2be-40cf-8326-318eca5e31fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'低風險': 3677, '中高風險': 524})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove index for string length > 512(模型需要)\n",
        "remove_index = []\n",
        "for i, x in enumerate(dat['text']):  \n",
        "  if len(x) > 512:\n",
        "    remove_index += [i]\n",
        "dat = dat.drop(remove_index, axis=0).reset_index(drop=True)\n",
        "print(len(remove_index))\n",
        "print(dat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzcuGHVYtFM3",
        "outputId": "09928f99-deb7-494d-ca1c-7543e302028f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "(4183, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data\n",
        "seed(1)\n",
        "train_index = sample(range(dat.shape[0]), int(0.8*dat.shape[0]))\n",
        "test_index = list(set(range(dat.shape[0])).difference(set(train_index)))\n",
        "valid_index = sample(train_index, int(0.2*len(train_index)))\n",
        "train_index = list(set(train_index).difference(set(valid_index)))\n",
        "dat_train = dat.iloc[train_index,]\n",
        "dat_valid = dat.iloc[valid_index,]\n",
        "dat_test = dat.iloc[test_index,]\n",
        "print('trian:', dat_train.shape) \n",
        "print('valid:', dat_valid.shape)\n",
        "print('test:', dat_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aE8eYok64vz",
        "outputId": "4b10d5fe-f696-4b9f-9eed-c6e966437b85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trian: (2677, 3)\n",
            "valid: (669, 3)\n",
            "test: (837, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('org:', Counter(dat_train['label']))\n",
        "# data augmentation\n",
        "mh_number = Counter(dat_train['label'])['中高風險'] # mid, hight\n",
        "dat_train_upsampling = dat_train[dat_train['label']=='中高風險'].sample(n=500-mh_number, replace=True, random_state=1)\n",
        "dat_train = pd.concat([dat_train, dat_train_upsampling])\n",
        "print('aug:', Counter(dat_train['label']))"
      ],
      "metadata": {
        "id": "mAQhtA8xDXEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21089a98-2216-4c0b-f3cf-c0bc4f84fd37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "org: Counter({'低風險': 2366, '中高風險': 311})\n",
            "aug: Counter({'低風險': 2366, '中高風險': 500})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saved data(模型需要)\n",
        "dat_train.loc[:,['text','label']].to_csv(os.path.join(path,'train.tsv'), sep=\"\\t\", index=False)\n",
        "dat_valid.loc[:,['text','label']].to_csv(os.path.join(path,'valid.tsv'), sep=\"\\t\", index=False)\n",
        "dat_test.loc[:,['text','label']].to_csv(os.path.join(path,'test.tsv'), sep=\"\\t\", index=False)"
      ],
      "metadata": {
        "id": "tx2DYU-MDXEV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ID = dat['ID'].tolist()\n",
        "len(all_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCuD4AUBp6sk",
        "outputId": "eb06f527-4a2e-40c2-e31e-ae1e84c5a584"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4183"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocess for BERT"
      ],
      "metadata": {
        "id": "ON3PHc6qvMEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "實作一個可以用來讀取訓練 / 測試集的 Dataset, 這是你需要徹底了解的部分。\n",
        "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors:\n",
        "- tokens_tensor: 兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
        "- segments_tensor: 可以用來識別兩個句子界限的 binary tensor\n",
        "- label_tensor: 將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
        "\"\"\"\n",
        "\n",
        "class FakeNewsDataset(Dataset):\n",
        "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
        "    def __init__(self, mode, tokenizer):\n",
        "        assert mode in [\"train\", \"valid\", \"test\"]  # 一般訓練你會需要 dev set\n",
        "        self.mode = mode\n",
        "        # 大數據你會需要用 iterator=True\n",
        "        self.df = pd.read_csv(os.path.join(path, f'{mode}.tsv'), sep=\"\\t\").fillna(\"\")\n",
        "        self.len = len(self.df)\n",
        "        self.label_map = {'低風險': 0, '中高風險': 1}\n",
        "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
        "    \n",
        "    # 定義回傳一筆訓練 / 測試數據的函式\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == \"test\":\n",
        "            # text_a, text_b = self.df.iloc[idx, :2].values\n",
        "            text = self.df.iloc[idx, 0] # 因為我們只有 text ,故不需要用.values\n",
        "            label_tensor = None\n",
        "        else:\n",
        "            text, label = self.df.iloc[idx, :].values\n",
        "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
        "            label_id = self.label_map[label]\n",
        "            label_tensor = torch.tensor(label_id)\n",
        "            \n",
        "        # # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
        "        # word_pieces = [\"[CLS]\"]\n",
        "        # tokens_a = self.tokenizer.tokenize(text_a)\n",
        "        # word_pieces += tokens_a + [\"[SEP]\"]\n",
        "        # len_a = len(word_pieces)\n",
        "        \n",
        "        # # 第二個句子的 BERT tokens\n",
        "        # tokens_b = self.tokenizer.tokenize(text_b)\n",
        "        # word_pieces += tokens_b + [\"[SEP]\"]\n",
        "        # len_b = len(word_pieces) - len_a\n",
        "\n",
        "        # # 如果只有一句話 BERT tokens\n",
        "        word_pieces = [\"[CLS]\"] # 開頭\n",
        "        tokens = self.tokenizer.tokenize(text) # output: list\n",
        "        word_pieces += tokens + [\"[SEP]\"] # 結尾\n",
        "        len_ = len(word_pieces)\n",
        "        \n",
        "        # 將整個 token 序列轉換成索引序列\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "        tokens_tensor = torch.tensor(ids)\n",
        "        \n",
        "        # 將第一句包含 [SEP] 的 token 位置設為 0, 其他為 1 表示第二句\n",
        "        # segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
        "        #                                 dtype=torch.long)\n",
        "        \n",
        "        # 將第一句包含 [SEP] 的 token 位置設為 0, 因為只有一句話故全都是 0\n",
        "        segments_tensor = torch.tensor([0] * len_, \n",
        "                                        dtype=torch.long)\n",
        "        \n",
        "        return (tokens_tensor, segments_tensor, label_tensor)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n"
      ],
      "metadata": {
        "id": "UqFQX55768D3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainset.df.iloc[0]['text']"
      ],
      "metadata": {
        "id": "qJmCHszI5UhR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
        "這個 DataLoader 吃我們上面定義的 `FakeNewsDataset`,\n",
        "回傳訓練 BERT 時會需要的 4 個 tensors:\n",
        "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
        "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
        "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
        "- label_ids       : (batch_size)\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "其中, pad_sequence 是將不同長度的 tensor 補齊到相同長度的函式，\n",
        "我們將 tokens_tensors 和 segments_tensors 分別傳給 pad_sequence 函式, 讓它們都補齊到 batch 中最長的序列的長度,\n",
        "這樣做的好處是可以讓整個 batch 進行矩陣運算，加速模型的訓練。\n",
        "'''\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
        "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
        "# - tokens_tensor\n",
        "# - segments_tensor\n",
        "# - label_tensor\n",
        "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
        "def create_mini_batch(samples):\n",
        "    tokens_tensors = [s[0] for s in samples]\n",
        "    segments_tensors = [s[1] for s in samples]\n",
        "    \n",
        "    # 測試集有 labels\n",
        "    if samples[0][2] is not None:\n",
        "        label_ids = torch.stack([s[2] for s in samples])\n",
        "    else:\n",
        "        label_ids = None\n",
        "    \n",
        "    # zero pad 到同一序列長度, 取 batch 中最長的作為 padding 到的最大長度\n",
        "    tokens_tensors = pad_sequence(tokens_tensors, \n",
        "                                  batch_first=True)\n",
        "    segments_tensors = pad_sequence(segments_tensors, \n",
        "                                    batch_first=True)\n",
        "    \n",
        "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
        "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
        "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
        "                                dtype=torch.long)\n",
        "    masks_tensors = masks_tensors.masked_fill(\n",
        "        tokens_tensors != 0, 1)\n",
        "    \n",
        "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
        "\n"
      ],
      "metadata": {
        "id": "q_w3RLGZyMbc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
        "BATCH_SIZE = 8\n",
        "# 初始化一個專門讀取訓練樣本的 Dataset, 使用中文 BERT 斷詞\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
        "                         collate_fn=create_mini_batch,\n",
        "                         shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "validset = FakeNewsDataset(\"valid\", tokenizer=tokenizer)\n",
        "validloader = DataLoader(validset, batch_size=BATCH_SIZE, \n",
        "                        collate_fn=create_mini_batch,\n",
        "                        shuffle=False, num_workers=0, pin_memory=True)"
      ],
      "metadata": {
        "id": "QfL14cgkokpO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.df\n",
        "# validset.df"
      ],
      "metadata": {
        "id": "E0vvLjMsOuH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假設有三個序列，分別是 [1, 2, 3], [4, 5], [6]\n",
        "seqs = [torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor([6])]\n",
        "\n",
        "# 使用 pad_sequence 將這三個序列補到相同的長度\n",
        "padded_seqs = pad_sequence(seqs, batch_first=True)\n",
        "padded_seqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD8s4btE5h1x",
        "outputId": "f307d2e5-0cf9-43df-bc5d-0cfc0d50c0b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 0],\n",
              "        [6, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(trainloader))\n",
        "\n",
        "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
        "\n",
        "print(f\"\"\"\n",
        "tokens_tensors.shape   = {tokens_tensors.shape} \n",
        "{tokens_tensors}\n",
        "------------------------\n",
        "segments_tensors.shape = {segments_tensors.shape}\n",
        "{segments_tensors}\n",
        "------------------------\n",
        "masks_tensors.shape    = {masks_tensors.shape}\n",
        "{masks_tensors}\n",
        "------------------------\n",
        "label_ids.shape        = {label_ids.shape}\n",
        "{label_ids}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD-yPu0eyNId",
        "outputId": "99a6c4f7-b367-4053-b0d2-4146683a2bf5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "tokens_tensors.shape   = torch.Size([8, 366]) \n",
            "tensor([[ 101, 6857,  855,  ...,    0,    0,    0],\n",
            "        [ 101, 6857,  855,  ..., 2692,  511,  102],\n",
            "        [ 101, 6857,  855,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 6857,  855,  ...,    0,    0,    0],\n",
            "        [ 101, 6857,  855,  ...,    0,    0,    0],\n",
            "        [ 101, 6857,  855,  ...,    0,    0,    0]])\n",
            "------------------------\n",
            "segments_tensors.shape = torch.Size([8, 366])\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]])\n",
            "------------------------\n",
            "masks_tensors.shape    = torch.Size([8, 366])\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "------------------------\n",
            "label_ids.shape        = torch.Size([8])\n",
            "tensor([0, 0, 0, 0, 1, 0, 0, 0])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 載入一個可以做中文多分類任務的模型, n_class = 2\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
        "# clear_output()\n",
        "\n",
        "# # high-level 顯示此模型裡的 modules\n",
        "# print(\"\"\"\n",
        "# name            module\n",
        "# ----------------------\"\"\")\n",
        "# for name, module in model.named_children():\n",
        "#     if name == \"bert\":\n",
        "#         for n, _ in module.named_children():\n",
        "#             print(f\"{name}:{n}\")\n",
        "#     else:\n",
        "#         print(\"{:15} {}\".format(name, module))"
      ],
      "metadata": {
        "id": "pMNFlMTD8UuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "n3cmt1uYvZWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"cuda\" only when GPUs are available.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# config\n",
        "config = BertConfig.from_pretrained(\"bert-base-chinese\")\n",
        "config.num_labels = 2\n",
        "# config.hidden_dropout_prob = 0.5\n",
        "\n",
        "# load model\n",
        "# pretrained_model_name = \"bert-base-chinese\"\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", config=config)\n",
        "model = model.to(device)\n",
        "\n",
        "# 使用 Adam Optim 更新整個分類模型的參數\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5) # weight_decay=1e-5 \n",
        "\n",
        "# For the classification task, we use cross-entropy as the measurement of performance.\n",
        "# weight = torch.tensor([1.0, 2.0]).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# hyper-parameters\n",
        "n_epochs = 30\n",
        "\n",
        "# Initialize trackers, these are not parameters and should not be changed\n",
        "stale = 0\n",
        "best_acc = 0\n",
        "best_loss = 1e6\n",
        "patience = 100\n",
        "\n",
        "train_loss_recode = []\n",
        "valid_loss_recode = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  \n",
        "  # ---------- Training ----------\n",
        "    # Make sure the model is in train mode before training.\n",
        "    model.train()\n",
        "\n",
        "    # These are used to record information in training.\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "\n",
        "    for data in tqdm(trainloader):\n",
        "      \n",
        "      # Forward the data. (Make sure data and model are on the same device.)\n",
        "      tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(input_ids=tokens_tensors, \n",
        "                      token_type_ids=segments_tensors, \n",
        "                      attention_mask=masks_tensors)\n",
        "      \n",
        "      # logits\n",
        "      logits = outputs[0]\n",
        "      # print(logits)\n",
        "\n",
        "      # loss\n",
        "      loss = criterion(logits, labels)\n",
        "      # print(loss)\n",
        "\n",
        "      # Gradients stored in the parameters in the previous step should be cleared out first.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Compute the gradients for parameters.\n",
        "      loss.backward()\n",
        "\n",
        "      # # Clip the gradient norms for stable training.\n",
        "      grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "      # Update the parameters with computed gradients.\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute the accuracy for current batch.\n",
        "      acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "      # Record the loss and accuracy.\n",
        "      train_loss.append(loss.item())\n",
        "      train_accs.append(acc)\n",
        "\n",
        "    train_loss = sum(train_loss) / len(train_loss)\n",
        "    train_acc = sum(train_accs) / len(train_accs)\n",
        "    \n",
        "    train_loss_recode += [train_loss]\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
        "    model.eval()\n",
        "\n",
        "    # These are used to record information in validation.\n",
        "    valid_loss = []\n",
        "    valid_accs = []\n",
        "\n",
        "    for data in tqdm(validloader):\n",
        "\n",
        "      # Forward the data. (Make sure data and model are on the same device.)\n",
        "      tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "      # We don't need gradient in validation.\n",
        "      # Using torch.no_grad() accelerates the forward process.\n",
        "      with torch.no_grad():\n",
        "        outputs = model(input_ids=tokens_tensors, \n",
        "                        token_type_ids=segments_tensors, \n",
        "                        attention_mask=masks_tensors) # labels\n",
        "        \n",
        "      # We can still compute the loss (but not the gradient).\n",
        "      logits = outputs[0]\n",
        "      \n",
        "      # We can still compute the loss (but not the gradient).\n",
        "      loss = criterion(logits, labels)\n",
        "      \n",
        "      # Compute the accuracy for current batch.\n",
        "      # print(logits.argmax(dim=-1))\n",
        "      # print(labels)\n",
        "      acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
        "      # print(acc)\n",
        "\n",
        "      # Record the loss and accuracy.\n",
        "      valid_loss.append(loss.item())\n",
        "      valid_accs.append(acc)\n",
        "  \n",
        "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
        "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "    valid_loss_recode += [valid_loss]\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # update logs\n",
        "    if valid_acc > best_acc: # , valid_loss < best_loss\n",
        "        with open(f\"./sample_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
        "    else:\n",
        "        with open(f\"./sample_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # save models\n",
        "    if valid_acc > best_acc: # valid_acc > best_acc, valid_loss < best_loss\n",
        "        print(f\"Best model found at epoch {epoch}, saving model\")\n",
        "        torch.save(model.state_dict(), f\"sample_best.ckpt\") # only save best to prevent output memory exceed error\n",
        "        best_acc = valid_acc\n",
        "        # best_loss = valid_loss\n",
        "        stale = 0\n",
        "    else:\n",
        "        stale += 1\n",
        "        if stale > patience:\n",
        "            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
        "            break\n",
        "\n"
      ],
      "metadata": {
        "id": "G4EeuwMP-C6u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "outputId": "fc55fabd-fc9d-4bef-9a0f-75d35b73c608"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 359/359 [01:07<00:00,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Train | 001/040 ] loss = 0.44858, acc = 0.82695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:06<00:00, 13.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Valid | 001/040 ] loss = 0.36953, acc = 0.86071\n",
            "[ Valid | 001/040 ] loss = 0.36953, acc = 0.86071 -> best\n",
            "Best model found at epoch 0, saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 9/359 [00:01<01:08,  5.11it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2cb05139e1dd>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;31m# Update the parameters with computed gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;31m# Compute the accuracy for current batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_foreach ops don't support autograd\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m     \u001b[0mgrouped_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_group_tensors_by_device_and_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m     for (device_params, device_grads, device_exp_avgs, device_exp_avg_sqs,\n\u001b[1;32m    427\u001b[0m          device_max_exp_avg_sqs, device_state_steps) in grouped_tensors.values():\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# override this method if your children class takes __init__ parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss curve\n",
        "plt.plot(range(1,31), train_loss_recode)\n",
        "plt.plot(range(1,31), valid_loss_recode)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('cross entropy loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e5BlITasQFva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read test data\n",
        "testset = FakeNewsDataset(\"test\", tokenizer=tokenizer)\n",
        "testloader = DataLoader(testset, batch_size=16, \n",
        "                        collate_fn=create_mini_batch)\n",
        "# next(iter(testloader))\n",
        "true_label = testset.df['label'].apply(lambda x: testset.label_map[x]).tolist() # 文字 -> label id(0 or 1)"
      ],
      "metadata": {
        "id": "q8Iw4ssMU7p5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model predict"
      ],
      "metadata": {
        "id": "Y86l2H9avhfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"cuda\" only when GPUs are available.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)\n",
        "model_best = model.to(device)\n",
        "model_best.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/屏東孤老/sample_best_109-111_up500.ckpt'))\n",
        "model_best.eval()\n",
        "\n",
        "# prediction = []\n",
        "pred_prob = []\n",
        "with torch.no_grad():\n",
        "    for data in testloader: # testloader, validloader, trainloader\n",
        "      data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
        "      tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
        "      outputs = model(input_ids=tokens_tensors, \n",
        "                      token_type_ids=segments_tensors, \n",
        "                      attention_mask=masks_tensors)\n",
        "      logits = outputs[0]\n",
        "      softmax_fn = nn.Softmax(dim=1)\n",
        "      output = softmax_fn(logits)\n",
        "      pred_prob += output.cpu().tolist() # saved probability\n",
        "      # _, pred = torch.max(output, 1) # 放 logits or output 皆可, 不影響輸出的 label\n",
        "      # prediction += pred.cpu().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJQysmXDT4db",
        "outputId": "749696c5-7e1c-4a27-f0bc-1f7a40d6a394"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def performance_fn(true_label, pred_label):\n",
        "  cm = confusion_matrix(true_label, pred_label)\n",
        "  TN, FP, FN, TP = cm.ravel() \n",
        "  TPR = TP / (TP+FN)\n",
        "  acc = (TP+TN) / (TP+FP+FN+TN)\n",
        "  print('Confusion matrix:\\n', cm)  \n",
        "  print(f'accuracy: {acc:.5f}, True positive rate: {TPR:.5f}')"
      ],
      "metadata": {
        "id": "L_WEAZsb_SoB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_label = validset.df['label'].apply(lambda x: validset.label_map[x]).tolist() # 文字 -> label id(0 or 1)\n",
        "\n",
        "# 0: 低風險, 1: 中高風險\n",
        "pred_prob_positive = [prob[1] for prob in pred_prob]\n",
        "pred_prob_positive = np.array(pred_prob_positive)\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(valid_label, pred_prob_positive, pos_label=1)\n",
        "print('auc:', metrics.auc(fpr, tpr))\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "mMYxv1FdJrNg",
        "outputId": "ab0052d3-f653-4033-fcc2-2b5343ec2626"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc: 0.7711973566308244\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(thresholds[tpr >= 0.4][0])\n",
        "print(thresholds[tpr >= 0.5][0])\n",
        "print(thresholds[tpr >= 0.6][0])\n",
        "print(thresholds[tpr >= 0.7][0])\n",
        "print(thresholds[tpr >= 0.8][0])\n",
        "print(thresholds[tpr >= 0.9][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKfuFuhM77bf",
        "outputId": "be557366-efed-4a29-aeeb-ae9f74a7d700"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16098061203956604\n",
            "0.13295593857765198\n",
            "0.10303357243537903\n",
            "0.06932202726602554\n",
            "0.04792077839374542\n",
            "0.03814150020480156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing result, up500.ckpt\n",
        "prediction = [1 if prob[1] > 0.16098061203956604 else 0 for prob in pred_prob]\n",
        "performance_fn(true_label, prediction) # left: true, top: pred, 0: 低風險, 1: 中高風險\n",
        "\n",
        "prediction = [1 if prob[1] > 0.13295593857765198 else 0 for prob in pred_prob]\n",
        "performance_fn(true_label, prediction)\n",
        "\n",
        "prediction = [1 if prob[1] > 0.10303357243537903 else 0 for prob in pred_prob]\n",
        "performance_fn(true_label, prediction)\n",
        "\n",
        "prediction = [1 if prob[1] > 0.06932202726602554 else 0 for prob in pred_prob]\n",
        "performance_fn(true_label, prediction)\n",
        "\n",
        "prediction = [1 if prob[1] > 0.04792077839374542 else 0 for prob in pred_prob]\n",
        "performance_fn(true_label, prediction)\n",
        "\n",
        "prediction = [1 if prob[1] > 0.03814150020480156 else 0 for prob in pred_prob]\n",
        "performance_fn(true_label, prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy74OWf4qdmQ",
        "outputId": "c68f76da-bfe0-4c0f-d0e7-a2cf570b816e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            " [[653  72]\n",
            " [ 65  47]]\n",
            "accuracy: 0.83632, True positive rate: 0.41964\n",
            "Confusion matrix:\n",
            " [[621 104]\n",
            " [ 56  56]]\n",
            "accuracy: 0.80884, True positive rate: 0.50000\n",
            "Confusion matrix:\n",
            " [[587 138]\n",
            " [ 44  68]]\n",
            "accuracy: 0.78256, True positive rate: 0.60714\n",
            "Confusion matrix:\n",
            " [[443 282]\n",
            " [ 29  83]]\n",
            "accuracy: 0.62843, True positive rate: 0.74107\n",
            "Confusion matrix:\n",
            " [[334 391]\n",
            " [ 15  97]]\n",
            "accuracy: 0.51493, True positive rate: 0.86607\n",
            "Confusion matrix:\n",
            " [[230 495]\n",
            " [  8 104]]\n",
            "accuracy: 0.39904, True positive rate: 0.92857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model performance"
      ],
      "metadata": {
        "id": "5bGmiU8rvoUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final prediction\n",
        "prediction = [1 if prob[1] > 0.16098061203956604 else 0 for prob in pred_prob]\n",
        "# performance table\n",
        "perf_df = dat_test.loc[:,['ID','label']]\n",
        "perf_df.columns = ['ID', 'my_label']\n",
        "perf_df.loc[:,'pred_label'] = prediction\n",
        "\n",
        "# read data\n",
        "tmp = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/屏東孤老/去識別化資料.xlsx',\n",
        "                    sheet_name='109-110個管系統個案資料')\n",
        "tmp2 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/屏東孤老/去識別化資料.xlsx',\n",
        "                     sheet_name='111年個管系統個案資料 ')\n",
        "# rename cols\n",
        "tmp = tmp.loc[:,['ID','級別']]\n",
        "tmp2 = tmp2.loc[:,['ID','危險分級']]\n",
        "tmp2.columns = ['ID', '級別']\n",
        "print(tmp.shape)\n",
        "print(tmp2.shape)\n",
        "# rowbind [tmp, tmp2]\n",
        "ID2they_label = pd.concat([tmp, tmp2]).reset_index(drop=True)\n",
        "ID2they_label = ID2they_label.drop_duplicates(subset=['ID'], keep='last')\n",
        "ID2they_label = ID2they_label[ID2they_label['ID'].isin(all_ID)].reset_index(drop=True) # 4183\n",
        "print(ID2they_label.shape)\n",
        "ID2they_label.loc[ID2they_label['級別'].str.contains('低危險'), '級別'] = '低風險'\n",
        "ID2they_label.loc[~ID2they_label['級別'].str.contains('低風險'), '級別'] = '中高風險'\n",
        "# update performance table\n",
        "perf_df = perf_df.merge(ID2they_label, how='left', on='ID') # left join to perf_df\n",
        "perf_df.columns = ['ID', 'my_label', 'pred_label', 'they_label']\n",
        "\n",
        "# rename labels(text -> id(0,1))\n",
        "label_map = {'低風險': 0, '中高風險': 1}\n",
        "perf_df['my_label'] = perf_df['my_label'].apply(lambda x: label_map[x]).tolist()\n",
        "perf_df['they_label'] = perf_df['they_label'].apply(lambda x: label_map[x]).tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1X5WWlUpbcS",
        "outputId": "e086fa1d-3dc4-4ee0-9d14-dae69600a480"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1635, 2)\n",
            "(3929, 2)\n",
            "(4183, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perf_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rnIu9TJFxh-6",
        "outputId": "5bb9d059-f676-4450-b031-cee5a0a7ff05"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ID  my_label  pred_label  they_label\n",
              "0  T210001         1           1           0\n",
              "1  T210004         0           0           0\n",
              "2  T113626         0           0           0\n",
              "3  T210013         0           0           0\n",
              "4  S210017         0           0           0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b2435c6-1e20-4f31-9f49-5da84008b06c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>my_label</th>\n",
              "      <th>pred_label</th>\n",
              "      <th>they_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T210001</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T210004</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T113626</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>T210013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S210017</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b2435c6-1e20-4f31-9f49-5da84008b06c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b2435c6-1e20-4f31-9f49-5da84008b06c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b2435c6-1e20-4f31-9f49-5da84008b06c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_label = perf_df['my_label'].tolist()\n",
        "pred_label = perf_df['pred_label'].tolist()\n",
        "they_label = perf_df['they_label'].tolist()\n",
        "\n",
        "performance_fn(my_label, pred_label)\n",
        "performance_fn(my_label, they_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4prqGOk2xnRS",
        "outputId": "b81271f8-a6a2-4377-946f-10d5d557826e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            " [[653  72]\n",
            " [ 65  47]]\n",
            "accuracy: 0.83632, True positive rate: 0.41964\n",
            "Confusion matrix:\n",
            " [[701  24]\n",
            " [ 95  17]]\n",
            "accuracy: 0.85783, True positive rate: 0.15179\n"
          ]
        }
      ]
    }
  ]
}